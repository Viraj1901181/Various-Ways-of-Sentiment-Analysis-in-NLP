# -*- coding: utf-8 -*-
"""PDF_Sentences_Sentiment Analysis_Dateextracted.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ntdX_xhdtAU-1NvS-KcVgPq7-HMNW0Oj

# Dependencies
"""

!pip install PyPDF2
!pip install tabula
!pip install textract
import nltk
import pandas as pd
import PyPDF2
import tabula
import textract
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

open_filename_WS = open("/content/Witness Statement Pack.pdf", 'rb')
#open_filename_DB = open(r"/content/Digital Bundle.pdf", 'rb')
#open_filename_ET = open("/content/ET unmarked.pdf", 'rb')


WS = PyPDF2.PdfFileReader(open_filename_WS)
#DB = PyPDF2.PdfFileReader(open_filename_DB)
#ET = PyPDF2.PdfFileReader(open_filename_ET)
#print (open_filename)

WS.getDocumentInfo()
#DB.getDocumentInfo()
#ET.getDocumentInfo()

total_pages_WS = WS.numPages
print(total_pages_WS)

#total_pages_DB = DB.numPages
#print(total_pages_DB)

#total_pages_ET = ET.numPages
#print(total_pages_ET)

"""# Datasets TEXT format"""

count_WS = 0
text_WS  = ''

# Lets loop through, to read each page from the pdf file
while(count_WS < total_pages_WS):
    # Get the specified number of pages in the document
    mani_pageWS  = WS.getPage(count_WS)
    # Process the next page
    count_WS += 1
    # Extract the text from the page
    text_WS += mani_pageWS.extractText()
print(text_WS)

nltk.download('punkt')

sentencesWS = nltk.sent_tokenize(text_WS)
#sentencesDB = nltk.sent_tokenize(text_DB)
#sentencesET = nltk.sent_tokenize(text_ET)

print(sentencesWS)

corpusWS = []

import re
for i in range(len(sentencesWS)):
    reviewWS = re.sub('[^0-9a-z^A-Z]',' ',sentencesWS[i])
    reviewWS = reviewWS.lower()
    reviewWS = reviewWS.split()
    reviewWS = ' '.join(reviewWS)
    corpusWS.append(reviewWS)
corpusWS

len(corpusWS)

corpusWS

sentenceWS=["sentenceWS"]

for x in corpusWS:
    sentenceWS.append(x)
    #print("Transcript: {}".format(result.alternatives[0].transcript))



with open("sentenceWS.csv", 'w') as myfile:
    for x in sentenceWS:
        myfile.write(x)
        myfile.write("\n")

#Sentiment Analysis of the PDF
#This is a set of Natural Language Processing (NLP) technique of analysing, identifying and categorizing opinions expressed in a piece of text, in order to determine whether the writer's attitude towards a particular topic, product, politics, services, brands etc. is positive, negative, or neutral.

!pip install vaderSentiment
import vaderSentiment
from collections import defaultdict
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob

"""#Lets decide which model we should use, between TextBlob and VADER for analysis of our text. We will therefore use TextBlob for its simplcity, and since VADER is specifically for analysis of social media data.
#TextBlob function - returns two properties
#Polarity: a float value which ranges from [-1.0 to 1.0] where 0 indicates neutral, +1 indicates most positive statement and -1 rindicates most negative statement.
#Subjectivity: a float value which ranges from [0.0 to 1.0] where 0.0 is most objective while 1.0 is most subjective. Subjective sentence expresses some personal opinios, views, beliefs, emotions, allegations, desires, beliefs, suspicions, and speculations where as objective refers to factual information.
"""

import pandas as pd

# To read the CSV file
df = pd.read_csv('/content/sentenceWS.csv')

from textblob import TextBlob

# The x in the lambda function is a row (because I set axis=1)
# Apply iterates the function accross the dataframe's rows
df['polarity'] = df.apply(lambda x: TextBlob(x['sentenceWS']).sentiment.polarity, axis=1)
df['subjectivity'] = df.apply(lambda x: TextBlob(x['sentenceWS']).sentiment.subjectivity, axis=1)

print(df)

!pip install vadersentiment
!pip install nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(sentence):
    score = analyser.polarity_scores(sentence)
    print("{:-<40} {} {} {} {}".format(sentence, str(score['neg']), str(score['neu']), str(score['pos']), str(score['compound'])))

sentiment_analyzer_scores("The phone is super cool.")

"""
cond3 = df['sentenceWS'].str.contains("EVENT | Event | event", na=False)
df = df[cond3]

print(df)
"""

from dateutil.parser import parse

def extract_date(sentence):
  sentence = str(sentence)
  result = parse(str(sentence), fuzzy_with_tokens=True)
  return str(result[0])


for i in range(len(df)):
  try:
    print(i," ",extract_date(df['sentenceWS'][i]))  
  except Exception:
    pass
  
# df['date_references'] = df['sentenceWS'].apply(extract_date)

# print(df)

"""# ELASTIC SEARCH INTEGRATION"""

!pip install json
!pip install requests_aws4auth
!pip install elasticsearch
import json
from elasticsearch import Elasticsearch
from elasticsearch import RequestsHttpConnection
from requests_aws4auth import AWS4Auth

service = 'es'

# credentials = boto3.Session().get_credentials()
awsauth = AWS4Auth("AKIAYNAPVMM4FYCW7I77", "qyzeHqsjFWGsFn6zfoRDZ0VfAxcbcSs6K43sOPOQ", "eu-west-2", service)

es = Elasticsearch(
    hosts=[{'host': "search-test-uldzkka4lukbz66x4nq2hqsvly.eu-west-2.es.amazonaws.com", 'port': 443}],
    http_auth=awsauth,
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection,
    request_timeout=30
)

print(es.info)

print(json.dumps(es.info(), indent=2))

index_name = "pdf_analysis_three"
id = 0


for index, row in df.iterrows():
  print(index)
  sentence = row['sentenceWS']
  pol = row['polarity']
  sub = row['subjectivity']
  doc = {
      'sentence': str(sentence),
      'polarity': str(pol),
      'subjectivity' : str(sub)
  }
  es.index(index=index_name, doc_type="_doc", id=id, body=doc)
  id = id + 1
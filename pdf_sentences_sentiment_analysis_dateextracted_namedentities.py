# -*- coding: utf-8 -*-
"""PDF_Sentences_Sentiment Analysis_Dateextracted_NamedEntities.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rtax_LPHivQ237hg6c8Mb4XYBDzza-xU

# Dependencies
"""

!pip install PyPDF2
!pip install tabula
!pip install textract
import nltk
import pandas as pd
import PyPDF2
import tabula
import textract
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

open_filename_WS = open("/content/Witness Statement Pack.pdf", 'rb')
#open_filename_DB = open(r"/content/Digital Bundle.pdf", 'rb')
#open_filename_ET = open("/content/ET unmarked.pdf", 'rb')


WS = PyPDF2.PdfFileReader(open_filename_WS)
#DB = PyPDF2.PdfFileReader(open_filename_DB)
#ET = PyPDF2.PdfFileReader(open_filename_ET)
#print (open_filename)

WS.getDocumentInfo()
#DB.getDocumentInfo()
#ET.getDocumentInfo()

total_pages_WS = WS.numPages
print(total_pages_WS)

#total_pages_DB = DB.numPages
#print(total_pages_DB)

#total_pages_ET = ET.numPages
#print(total_pages_ET)

"""# Datasets TEXT format"""

count_WS = 0
text_WS  = ''

# Lets loop through, to read each page from the pdf file
while(count_WS < total_pages_WS):
    # Get the specified number of pages in the document
    mani_pageWS  = WS.getPage(count_WS)
    # Process the next page
    count_WS += 1
    # Extract the text from the page
    text_WS += mani_pageWS.extractText()
print(text_WS)

nltk.download('punkt')

sentencesWS = nltk.sent_tokenize(text_WS)
#sentencesDB = nltk.sent_tokenize(text_DB)
#sentencesET = nltk.sent_tokenize(text_ET)

print(sentencesWS)

corpusWS = []

import re
for i in range(len(sentencesWS)):
    reviewWS = re.sub('[^0-9a-z^A-Z]',' ',sentencesWS[i])
    reviewWS = reviewWS.lower()
    reviewWS = reviewWS.split()
    reviewWS = ' '.join(reviewWS)
    corpusWS.append(reviewWS)
corpusWS

len(corpusWS)

corpusWS

sentenceWS=["sentenceWS"]

for x in corpusWS:
    sentenceWS.append(x)
    #print("Transcript: {}".format(result.alternatives[0].transcript))



with open("sentenceWS.csv", 'w') as myfile:
    for x in sentenceWS:
        myfile.write(x)
        myfile.write("\n")

#Sentiment Analysis of the PDF
#This is a set of Natural Language Processing (NLP) technique of analysing, identifying and categorizing opinions expressed in a piece of text, in order to determine whether the writer's attitude towards a particular topic, product, politics, services, brands etc. is positive, negative, or neutral.

!pip install vaderSentiment
import vaderSentiment
from collections import defaultdict
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob

"""#Lets decide which model we should use, between TextBlob and VADER for analysis of our text. We will therefore use TextBlob for its simplcity, and since VADER is specifically for analysis of social media data.
#TextBlob function - returns two properties
#Polarity: a float value which ranges from [-1.0 to 1.0] where 0 indicates neutral, +1 indicates most positive statement and -1 rindicates most negative statement.
#Subjectivity: a float value which ranges from [0.0 to 1.0] where 0.0 is most objective while 1.0 is most subjective. Subjective sentence expresses some personal opinios, views, beliefs, emotions, allegations, desires, beliefs, suspicions, and speculations where as objective refers to factual information.
"""

df2 = pd.read_csv('/content/sentenceWS.csv')

df2.head()

display(df2)

"""# Step 2: Extract named entities"""

def extract_named_ents(text):
    """Extract named entities, and beginning, middle and end idx using spaCy's out-of-the-box model. 
    
    Keyword arguments:
    text -- the actual text source from which to extract entities
    
    """
    return [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in nlp(text).ents]

def add_named_ents(df):
    """Create new column in data frame with named entity tuple extracted.
    
    Keyword arguments:
    df -- a dataframe object
    
    """
    df2['named_ents'] = df2['sentenceWS'].apply(extract_named_ents)

!pip install spacy
import spacy
nlp = spacy.load('en_core_web_sm')
add_named_ents(df2)
display(df2)

"""# Step 3: Extract all nouns"""

def extract_nouns(text):
    """Extract a few types of nouns, and beginning, middle and end idx using spaCy's POS (part of speech) tagger. 
    
    Keyword arguments:
    text -- the actual text source from which to extract entities
    
    """
    keep_pos = ['PROPN', 'NOUN']
    return [(tok.text, tok.idx, tok.idx+len(tok.text), tok.pos_) for tok in nlp(text) if tok.pos_ in keep_pos]

def add_nouns(df):
    """Create new column in data frame with nouns extracted.
    
    Keyword arguments:
    df -- a dataframe object
    
    """
    df2['nouns'] = df2['sentenceWS'].apply(extract_nouns)

add_nouns(df2)
display(df2)

'''
for i in range(len(df2)):
  spacy.displacy.render(nlp(df2['sentenceWS'][i]), jupyter=True) 
  
# generating raw-markup using spacy's built-in renderer

'''

def custom_render(doc, df, column, options={}, page=False, minify=False, idx=0):
    """Overload the spaCy built-in rendering to allow custom part-of-speech (POS) tags.
    
    Keyword arguments:
    doc -- a spaCy nlp doc object
    df -- a pandas dataframe object
    column -- the name of of a column of interest in the dataframe
    options -- various options to feed into the spaCy renderer, including colors
    page -- rendering markup as full HTML page (default False)
    minify -- for compact HTML (default False)
    idx -- index for specific query or doc in dataframe (default 0)
    
    """
    renderer, converter = EntityRenderer, parse_custom_ents
    renderer = renderer(options=options)
    parsed = [converter(doc, df=df, idx=idx, column=column)]
    html = renderer.render(parsed, page=page, minify=minify).strip()  
    return display(HTML(html))

def parse_custom_ents(doc, df, idx, column):
    """Parse custom entity types that aren't in the original spaCy module.
    
    Keyword arguments:
    doc -- a spaCy nlp doc object
    df -- a pandas dataframe object
    idx -- index for specific query or doc in dataframe
    column -- the name of of a column of interest in the dataframe
    
    """
    if column in df.columns:
        entities = df[column][idx]
        ents = [{'start': ent[1], 'end': ent[2], 'label': ent[3]} 
                for ent in entities]
    else:
        ents = [{'start': ent.start_char, 'end': ent.end_char, 'label': ent.label_}
            for ent in doc.ents]
    return {'text': doc.text, 'ents': ents, 'title': None}

def render_entities(idx, df, options={}, column='named_ents'):
    """A wrapper function to get text from a dataframe and render it visually in jupyter notebooks
    
    Keyword arguments:
    idx -- index for specific query or doc in dataframe (default 0)
    df -- a pandas dataframe object
    options -- various options to feed into the spaCy renderer, including colors
    column -- the name of of a column of interest in the dataframe (default 'named_ents')
    
    """
    text = df['text'][idx]
    custom_render(nlp(text), df=df, column=column, options=options, idx=idx)

import pandas as pd

# To read the CSV file
df = pd.read_csv('/content/sentenceWS.csv')

from textblob import TextBlob

# The x in the lambda function is a row (because I set axis=1)
# Apply iterates the function accross the dataframe's rows
df['polarity'] = df.apply(lambda x: TextBlob(x['sentenceWS']).sentiment.polarity, axis=1)
df['subjectivity'] = df.apply(lambda x: TextBlob(x['sentenceWS']).sentiment.subjectivity, axis=1)

print(df)

from dateutil.parser import parse

def extract_date(sentence):
  sentence = str(sentence)
  result = parse(str(sentence), fuzzy_with_tokens=True)
  return str(result[0])


for i in range(len(df)):
  try:
    print(i," ",extract_date(df['sentenceWS'][i]))  
  except Exception:
    pass
  
# df['date_references'] = df['sentenceWS'].apply(extract_date)

# print(df)

df2['Date']="No Date"
from dateutil.parser import parse

def extract_date(sentence):
  sentence = str(sentence)
  result = parse(str(sentence), fuzzy_with_tokens=True)
  return str(result[0])


for i in range(len(df2)):
  try:
    #print(i," ",extract_date(df2['sentenceWS'][i]))
    df2['Date'][i] = extract_date(df2['sentenceWS'][i]) 
  except Exception:
    pass
  
df2

df2

# colors for additional part of speech tags we want to visualize
options = {
    'colors': {'COMPOUND': '#FE6BFE', 'PROPN': '#18CFE6', 'NOUN': '#18CFE6', 'NP': '#1EECA6', 'ENTITY': '#FF8800'}
}

'''
column = 'named_ents'
for i in range(len(df)):
  try:
    render_entities(i, df2, options=options, column=column) # take a look at one of the abstracts
  except Exception:
    pass
'''

def extract_noun_phrases(text):
    """Combine noun phrases. 
    
    Keyword arguments:
    text -- the actual text source from which to extract entities
    
    """
    return [(chunk.text, chunk.start_char, chunk.end_char, chunk.label_) for chunk in nlp(text).noun_chunks]

def add_noun_phrases(df):
    """Create new column in data frame with noun phrases.
    
    Keyword arguments:
    df -- a dataframe object
    
    """
    df2['noun_phrases'] = df2['sentenceWS'].apply(extract_noun_phrases)

add_noun_phrases(df2)
display(df2)

"""# Step 6: Extract compound noun phrases"""

def extract_compounds(text):
    """Extract compound noun phrases with beginning and end idxs. 
    
    Keyword arguments:
    text -- the actual text source from which to extract entities
    
    """
    comp_idx = 0
    compound = []
    compound_nps = []
    tok_idx = 0
    for idx, tok in enumerate(nlp(text)):
        if tok.dep_ == 'compound':

            # capture hyphenated compounds
            children = ''.join([c.text for c in tok.children])
            if '-' in children:
                compound.append(''.join([children, tok.text]))
            else:
                compound.append(tok.text)

            # remember starting index of first child in compound or word
            try:
                tok_idx = [c for c in tok.children][0].idx
            except IndexError:
                if len(compound) == 1:
                    tok_idx = tok.idx
            comp_idx = tok.i

        # append the last word in a compound phrase
        if tok.i - comp_idx == 1:
            compound.append(tok.text)
            if len(compound) > 1: 
                compound = ' '.join(compound)
                compound_nps.append((compound, tok_idx, tok_idx+len(compound), 'COMPOUND'))

            # reset parameters
            tok_idx = 0 
            compound = []

    return compound_nps

def add_compounds(df):
    """Create new column in data frame with compound noun phrases.
    
    Keyword arguments:
    df -- a dataframe object
    
    """
    df2['compounds'] = df2['sentenceWS'].apply(extract_compounds)

add_compounds(df2)
display(df2)

"""# Step 7: Combine entities and compound noun phrases"""

def extract_comp_nouns(row_series, cols=[]):
    """Combine compound noun phrases and entities. 
    
    Keyword arguments:
    row_series -- a Pandas Series object
    
    """
    return {noun_tuple[0] for col in cols for noun_tuple in row_series[col]}

def add_comp_nouns(df, cols=[]):
    """Create new column in data frame with merged entities.
    
    Keyword arguments:
    df -- a dataframe object
    cols -- a list of column names that need to be merged
    
    """
    df2['comp_nouns'] = df2.apply(extract_comp_nouns, axis=1, cols=cols)

cols = ['nouns', 'compounds']
add_comp_nouns(df2, cols=cols)
display(df2)

for i in range(len(df)):
  try:
    print(i," ",extract_date(df['sentenceWS'][i]))  
  except Exception:
    pass
  
# df['date_references'] = df['sentenceWS'].apply(extract_date)

# print(df)

import spacy  

nlp = spacy.load('en')
for i in range(len(df)):
  try:
      print(i," ",sents = nlp(str(df['sentenceWS']))
      [ee for ee in sents.ents if ee.label_ == 'PERSON']))  
  except Exception:
    pass

print(df)

df['Date']="1"
from dateutil.parser import parse

def extract_date(sentence):
  sentence = str(sentence)
  result = parse(str(sentence), fuzzy_with_tokens=True)
  return str(result[0])


for i in range(len(df)):
  try:
    df['Date'][i] = extract_date(df['sentenceWS'][i]) 
  except Exception:
    pass
  
df

!pip install spacy
import spacy
nlp = spacy.load("en")

for i in df['sentenceWS']:
  doc = nlp(i)
  for entity in doc.ents:
    print((entity.text))
    print("\n")

!pip install nltk
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

lemmatizer = WordNetLemmatizer()
sentences = []
df['sentenceWS1']="No words"

df

nltk.download('wordnet')
nltk.download('stopwords')
for i in range(len(df)):
    words = nltk.word_tokenize(df['sentenceWS'][i])
    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]
    df['sentenceWS1'][i] = ' '.join(words)

df

df['sentenceWS'][0]

df['sentenceWS1'][0]

"""# CUSTOM NAMED ENTITY RECOGNITION"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

import pandas as pd
import spacy
from spacy.displacy.render import EntityRenderer
from IPython.core.display import display, HTML

def custom_render(doc, df, column, options={}, page=False, minify=False, idx=0):
    """Overload the spaCy built-in rendering to allow custom part-of-speech (POS) tags.
    
    Keyword arguments:
    doc -- a spaCy nlp doc object
    df -- a pandas dataframe object
    column -- the name of of a column of interest in the dataframe
    options -- various options to feed into the spaCy renderer, including colors
    page -- rendering markup as full HTML page (default False)
    minify -- for compact HTML (default False)
    idx -- index for specific query or doc in dataframe (default 0)
    
    """
    renderer, converter = EntityRenderer, parse_custom_ents
    renderer = renderer(options=options)
    parsed = [converter(doc, df=df, idx=idx, column=column)]
    html = renderer.render(parsed, page=page, minify=minify).strip()  
    return display(HTML(html))

def parse_custom_ents(doc, df, idx, column):
    """Parse custom entity types that aren't in the original spaCy module.
    
    Keyword arguments:
    doc -- a spaCy nlp doc object
    df -- a pandas dataframe object
    idx -- index for specific query or doc in dataframe
    column -- the name of of a column of interest in the dataframe
    
    """
    if column in df.columns:
        entities = df[column][idx]
        ents = [{'start': ent[1], 'end': ent[2], 'label': ent[3]} 
                for ent in entities]
    else:
        ents = [{'start': ent.start_char, 'end': ent.end_char, 'label': ent.label_}
            for ent in doc.ents]
    return {'text': doc.text, 'ents': ents, 'title': None}

def render_entities(idx, df, options={}, column='named_ents'):
    """A wrapper function to get text from a dataframe and render it visually in jupyter notebooks
    
    Keyword arguments:
    idx -- index for specific query or doc in dataframe (default 0)
    df -- a pandas dataframe object
    options -- various options to feed into the spaCy renderer, including colors
    column -- the name of of a column of interest in the dataframe (default 'named_ents')
    
    """
    text = df['text'][idx]
    custom_render(nlp(text), df=df, column=column, options=options, idx=idx)

# colors for additional part of speech tags we want to visualize
options = {
    'colors': {'COMPOUND': '#FE6BFE', 'PROPN': '#18CFE6', 'NOUN': '#18CFE6', 'NP': '#1EECA6', 'ENTITY': '#FF8800'}
}

pd.set_option('display.max_rows', 10) # edit how jupyter will render our pandas dataframes
pd.options.mode.chained_assignment = None # prevent warning about working on a copy of a dataframe

!pip install en_core_web_lg
nlp = spacy.load('en_core_web_sm')
#nlp = spacy.load('en_core_web_lg')

# Commented out IPython magic to ensure Python compatibility.
# for manipulating dataframes
import pandas as pd
# for natural language processing: named entity recognition
import spacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()
# for visualizations
# %matplotlib inline

tokens = nlp(''.join(str(df['sentenceWS'].tolist())))

items = [x.text for x in tokens.ents]
Counter(items).most_common(20)

person_list = []
for ent in tokens.ents:
    if ent.label_ == 'PERSON':
        person_list.append(ent.text)
        
person_counts = Counter(person_list).most_common(20)
df_person = pd.DataFrame(person_counts, columns =['text', 'count'])

df_person.head()

norp_list = []
for ent in tokens.ents:
    if ent.label_ == 'NORP':
        norp_list.append(ent.text)
        
norp_counts = Counter(norp_list).most_common(20)
df_norp = pd.DataFrame(norp_counts, columns =['text', 'count'])

df_norp.head()

df_norp.plot.barh(x='text', y='count', title="Nationalities, Religious, and Political Groups", figsize=(10,8)).invert_yaxis()

"""# TDS Trial NER

"""

!pip install re
import re

# date patern like “Month Day, Year”
for m in re.finditer(r’\b\w{3,10}\b \d{1,2}, \d{4,4}’, txt):
print(‘%02d-%02d: %s’ % (m.start(), m.end(), m.group(0)))
abstract=txt[max(m.start()-50,0): min(m.end()+50,len_txt)]
print(f’Abstract:\n {abstract}\n’)
# company name in single quotes after word between
for m in re.finditer(r’\bbetween\b [\’][A-Za-z\s\.\&\)\(]+[\’] \band\b [\’][A-Za-z\s\.\&\)\(]+[\’] ‘, txt):
print(‘%02d-%02d: %s’ % (m.start(), m.end(), m.group(0)))
a = re.search(r’\b(and)\b’, m.group(0))
conpany_name1=(m.group(0)[:a.start()].split(‘ ‘, 1)[1])
conpany_name2=(m.group(0)[a.start():].split(‘ ‘, 1)[1])
print (“Company 1: “, conpany_name1)
print (“Company 2: “, conpany_name2)
#match new line // Blank line
a = re.search(r’(\n|\r)’, txt)
print(a)

print(df)

"""# ELASTIC SEARCH INTEGRATION"""

!pip install json
!pip install requests_aws4auth
!pip install elasticsearch
import json
from elasticsearch import Elasticsearch
from elasticsearch import RequestsHttpConnection
from requests_aws4auth import AWS4Auth

service = 'es'

# credentials = boto3.Session().get_credentials()
awsauth = AWS4Auth("AKIAYNAPVMM4FYCW7I77", "qyzeHqsjFWGsFn6zfoRDZ0VfAxcbcSs6K43sOPOQ", "eu-west-2", service)

es = Elasticsearch(
    hosts=[{'host': "search-test-uldzkka4lukbz66x4nq2hqsvly.eu-west-2.es.amazonaws.com", 'port': 443}],
    http_auth=awsauth,
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection,
    request_timeout=30
)

print(es.info)

print(json.dumps(es.info(), indent=2))

index_name = "pdf_analysis_three"
id = 0


for index, row in df.iterrows():
  print(index)
  sentence = row['sentenceWS']
  pol = row['polarity']
  sub = row['subjectivity']
  doc = {
      'sentence': str(sentence),
      'polarity': str(pol),
      'subjectivity' : str(sub)
  }
  es.index(index=index_name, doc_type="_doc", id=id, body=doc)
  id = id + 1